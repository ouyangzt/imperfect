---
title: "When Is An Imperfect Test Useful?"
date: "March 22, 2017"
author: "Song S. Qian"
output: pdf_document
bibliography: "~/Dropbox/LaTeX/maintxt.bib"
---

# Initial Setup

In the hidden R code chunk, I load (and install, if not already installed) needed packages and set up working directory.  The working directory should have a sub-folder named `Data` for raw data file(s) and a folder named `Figures` for exporting figures.  
```{r, echo=F, message=FALSE}
require(lattice)
lattice::trellis.par.set(lattice::col.whitebg())
knitr::opts_chunk$set(warning=F, prompt=TRUE, tidy=TRUE, 
                      fig.width=7, fig.height=4, fig.path='Figs/',
                      echo=TRUE, warning=F)

packages<-function(x, repos="http://cran.r-project.org", ...){
  x<-as.character(match.call()[[2]])
  if (!require(x,character.only=TRUE)){
    install.packages(pkgs=x, repos=repos, ...)
    require(x,character.only=TRUE)
  }
}
base <- getwd()
dataDIR <- paste(base, "Data", sep="/")
plotDIR <- paste(base, "Figs", sep="/")

packages(arm)
packages(lattice)
packages(tikzDevice)
packages(reshape2)
packages(rstan)
packages(rv)

rstan_options(auto_write = TRUE)
options(mc.cores = min(c(parallel::detectCores(), 8)))

nchains <-  min(c(parallel::detectCores(), 8))
```

# Problem Setup
A typical genetic test for the presence of certain genes or gene fragments is imperfect: a test can have false positive and false negative.  For a test to be useful, we need to translate the test result (positive or negative) into a conditional probability: probability that the gene is present when a test returns a positive result, and the probability that the gene is absent upon a negative result.  Statistically, when using such a test, we have two sets of data.  One set of data is generated by the test developer to quantify the false positive and false negative probability.  We normally don't have access of these data.  But we can imaging that the false positive probability is estimated by testing a number of subject without the target gene present.  The proportion of positive result is an estimate of the false positive probability.  Likewise, the false negative probability can be estimated by testing a number of subject known to have the target gene present.  The proportion of negative results is an estimate of the false negative probability.  

When the test is used on a subject, we do not know whether the target gene is present or not. We only know the test result to be positive or negative.  From the result, we want to learn the conditional probability that the gene is present given the test result is positive (or the probability that gene is absent given the result is negative).  This conditional probability can be easily expressed using the Bayes rule.  To simplify the notation, we use  "+" to represent a positive test result and "-" a negative result.  Also, we use $p$ and $a$ to represent whether the gene is present or absent, respectively.  A false positive is represented by $+|a$ and a false negative is $-|p$ The quantities of interest are $\Pr(p|+)$ and $\Pr(a|-)$.  Using the Bayes' theorem, we have:
\begin{equation}
\Pr(p|+) = \frac{\Pr(p)\Pr(+|p)}{\Pr(p)\Pr(+|p)+\Pr(a)\Pr(+|a)}
\label{eq:pos}
\end{equation}
and 
\begin{equation}
\Pr(a|-) = \frac{\Pr(a)\Pr(-|a)}{\Pr(a)\Pr(-|a)+\Pr(p)\Pr(-|p)}
\label{eq:neg}
\end{equation}
These two equations are similar and we discuss equation \eqref{eq:pos} in detail; the result for equation \eqref{eq:neg} can be obtained in similar ways.

In equation \ref{eq:pos}, $\Pr(+|p) = 1-\Pr(-|p)$ is the power of the test (equals 1 minus false negative probability) and $\Pr(+|a)$ is the false positive probability.  In order to calculate the quantity of interest ($\Pr(p|+)$), we also need to know $\Pr(p)$ (and $\Pr(a)= 1-\Pr(p)$), the prevalence of the gene in the population.  This is often the quantity we want to learn, for example, from the results of testing individual animals for the presence of a disease gene.  In other words, we test individual animals in order to learn about the prevalence of the disease; but we need to know the prevalence of the disease in order to properly interpret the test result.  

# When Population Prevalence is Known

When we know $\Pr(p)$ for a given population, we are interested in identify individuals with the disease.  In this situation, we want to evaluate whether the test is useful -- defined by the posterior probability $\Pr(p|+)$.  If $\Pr(p|+) \leq 0.5$, the test provides no information.  That is, for a test to be useful, we need to have:
\begin{equation}
 \frac{\Pr(p)\Pr(+|p)}{\Pr(p)\Pr(+|p)+\Pr(a)\Pr(+|a)} > 0.5
\label{eq:ineq1}
\end{equation}
Or:
\[\Pr(+|a) < \frac{\Pr(p)}{1-\Pr(p)}(1-\Pr(-|p))\]

In general, if we want $\Pr(p|+) > \gamma$ and $1 > \gamma > 0.5$, the false positive probability must be:

\[ 
   \Pr(+|a) < \frac{1-\gamma}{\gamma}  \frac{\Pr(p)}{1-\Pr(p)}(1-\Pr(-|p))
  \]

We can graphically present the condition:
```{r, fig.width=5.75, fig.height=3.75, fig.align='center', tidy=T, message=FALSE}
##tikz(file=paste(plotDIR, "BayesTest1.tex", sep="/"), height = 3., width=5.75, standAlone = F)
par(mfrow=c(1,2), mar=c(4,3,1,0.5), mgp=c(1.75,0.125,0), las=1, tck=0.01)
plot(c(0,1), c(0,1), xlab="false negative", ylab="false positive", 
     sub="Pr(p)=0.2", type="n", ylim=c(0,0.3))
curve((0.2/0.8)*(1-x), add=TRUE)
curve((0.25/0.75)*(0.2/0.8)*(1-x), col="red", add=TRUE)
curve((0.1/0.9)*(0.2/0.8)*(1-x), col="blue", add=TRUE)
plot(c(0,1), c(0,1), xlab="false negative", ylab="false positive", 
     sub="Pr(p)=0.1", type="n", ylim=c(0,0.3))
curve((0.1/0.9)*(1-x), add=TRUE)
curve((0.25/0.75)*(0.1/0.9)*(1-x), col="red", add=TRUE)
curve((0.1/0.9)*(0.1/0.9)*(1-x), col="blue", add=TRUE)
legend(x=0.1, y=0.25, lty=1, col=c("black", "red","blue"), legend=c("$\\gamma=1$", "$\\gamma=3$", "$\\gamma=9$"), bty="n")
##dev.off()
```

The acceptable combination of $\Pr(+|a)$ and $\Pr(-|p)$ is in the area below each of the lines.   
When the prevalence of the disease (gene, ...) is low (in this example, 0.1), a positive result is useful only when the false positive rate is very low and false negative rate is not too high.  In other words, to make a positive result useful when the prevalence of the disease is low, we can accept a test with a reasonable false negative probability, as long as the false positive probability is very low.  In fact, if the false positive rate is 0, a positive result is always a definite indicator of the presence of the disease.

# When Population Prevalence is Unknown
When $\Pr(p)$ is unknown, the nature of the problem changed.  Instead of focusing on the individual tested, we are now interested in estimating the prevalence based on testing results.  We discuss the estimation problem in two steps.  First, let's assume that the test is perfect (i.e., $\Pr(+|a) = \Pr(-|p) = 0$).  A perfect test suggests that $\Pr(p|+) = 1$.  That is, we can treat a positive result as a definite confirmation of presence of the agent of interest.  If a number of subjects are tested, the data we have is the number of presences and the number of absences.  Given the unknown prevalence, we naturally model the data using the binomial model. Let the number of positive results be $y$, and the total number of tests be $n$.  The binomial model is
$$
y \sim Bin(\theta, n)
$$
where $\theta=\Pr(p)$ is the parameter of interest.  In classical statistics, this model leads to a likelihood function:
$$
L(y|\theta) \propto \theta^y(1-\theta)^{n-y}
$$
The maximum likelihood estimator is $\hat{\theta} = y/n$.  

```{r, fig.width=4.5, fig.height=3.75, fig.align="center", tidy=T, message=FALSE, sanitize=T}
##   tikz(file=paste(plotDIR, "Betas.tex", sep="/"), height=3.5, width=5,
##        standAlone = F)
    par(mar=c(3,2,1,0), mgp=c(1.25,0.125,0), las=1, tck=0.01)
    plot(c(0,1), c(0, 3), type="n", xlab="", ylab="")
    curve(dbeta(x, 0.5,0.5), add=T, col=1)
    curve(dbeta(x, 1,1), add=T, col=2)
    curve(dbeta(x, 1,5), add=T, col=3)
    curve(dbeta(x, 2,3), add=T, col=4)
    curve(dbeta(x, 5,1), add=T, col=5)
    legend(x=0.25,y=3, lty=1, col=1:5, 
           legend = c("$\\alpha=\\beta=0.5$","$\\alpha=\\beta=1$",
                      "$\\alpha=1,\\beta=5$","$\\alpha=2,\\beta=3$",
                      "$\\alpha=5,\\beta=1$"), bty="n")
##    dev.off()
```
If we have prior information about $\theta$, a common way of summarizing the knowledge is to use a Beta distribution:
\[
\theta \sim beta(\alpha, \beta)
\]
The density function of the beta distribution is proportional to $\theta^{\alpha-1}(1-\theta)^{\beta-1}$. Using the Bayes' theorem, the density of the posterior distribution of $\theta$ is:
\[
\pi(\theta|y) \propto \theta^{\alpha-1}(1-\theta)^{\beta-1}\times \theta^y(1-\theta)^{n-y}
\]
This density is a beta distribution with parameter $\alpha+y$ and $\beta+n-y$.  Typically, we interpret the prior distribution as as estimate of the prevalence based on observing $\alpha$ positives and $\beta$ negatives.  After observing $y$ positives and $n-y$ negatives, our new estimate is now based on the total number of positives and negatives, combining information from the prior and the data.  

Second, we now expand the Bayesian model to an imperfect test.  As in the perfect test situation, the data we have are still the number of positives and the total number of tests.  The number of positives is still modeled by a binomial distribution.  However, for an imperfect test, the binomial distribution parameter is no longer simple $\theta$.  Let $f_p$ and $f_n$ be the false positive and false negative probabilities, respectively.  The probability of observing a positive result is now $\theta^* = \theta(1-f_n) + (1-\theta)f_p$.  As a result, the likelihood function is now
\[
L(y|\theta) = \left [\theta(1-f_n) + (1-\theta)f_p\right ]^y\left [1-\theta(1-f_n) - (1-\theta)f_p\right ]^{n-y}
\]
If we use the same prior for $\theta$, the posterior is now a far more complicated function:
\begin{equation}
\pi(\theta|y) \propto \theta^{\alpha-1}(1-\theta)^{\beta-1}\times \left [\theta(1-f_n) + (1-\theta)f_p\right ]^y\left [1-\theta(1-f_n) - (1-\theta)f_p\right ]^{n-y}
\label{eq:post1}
\end{equation}
It is not obvious to us whether this posterior distribution can be represented by one of the commonly used statistical probability distributions.  However, we can numerically approximate the density function by evaluating the term in the right-hand-side of equation \eqref{eq:post1} over a grid of $\theta$ (between 0 and 1) if $f_n$ and $f_p$ are known.

```{r, fig.width=4, fig.align="center", fig.height=3.75, tidy=TRUE, message=FALSE}
postTheta<- function(y = 5,n = 20,fn = 0.05, fp = 0.02, alpha= 2, beta= 18, theta= seq(0.0, 0.6, 0.001)){
post <- (theta^(alpha-1))*((1-theta)^(beta-1))*((theta*(1-fn)+(1-theta)*fp)^y)*((1-theta*(1-fn)-(1-theta)*fp)^(n-y))
post <- post/(sum(post*0.001))
par(mar=c(3,3,1,0.5), mgp=c(1.25,0.125,0), las=1, tck=0.01)
plot(theta, post, type="l", ylim=c(0, max(dbeta(theta, alpha, beta), post)), 
     xlab="$\\theta$", ylab="$\\pi(\\theta|y)$")
curve(dbeta(x, alpha,beta), add=T, lty=2, col="red")
abline(v=y/n, lty=3, col="blue")
invisible(cbind(theta, post))
}
## Figure 2
tikz(file=paste(plotDIR, "BayesTest2.tex", sep="/"), height=3, width=4.5, standAlone = F)
output <- postTheta()
legend(x=0.35, y=6.5, col=c("red","black","blue"), lty=c(2,1,3), legend = c("prior", "posterior","data"), 
       bty="n")
dev.off()
```
In the above example, we have a prior distribution of $\theta$ to be beta(2,18) and the data are 5 positives in 20 tests.  The prior mean of $\theta$ is 0.1 and the MLE of $\theta$ is 0.25.  The posterior peak is at $\theta=$ `r output[,1][output[,2]==max(output[,2])]`, between the prior mean of 0.1 and the MLE of 0.25.   

Alternatively, we can use the modern Bayesian computation method, the Markov Chain Monte Carlo simulation.  
```{r, message=FALSE, tidy=TRUE, warning=FALSE}
stan.input <- function(y=5, n=20, alpha=2, beta=18, fn=0.05, fp=0.02, nchains=4)
{
  inits <- list()
  bugs.data <- list(y=y, n=n, alpha=alpha, beta=beta, fn=fn, fp=fp)
  for (i in 1:nchains) 
    inits[[i]] <- list(theta=runif(1))
  para <- c("theta")
  return(list(para=para, data=bugs.data, inits=inits, n.chains=nchains, model = "
         data{
          int n;
          int y;
          real alpha;
          real beta;
          real<lower=0,upper=1> fn;
          real<lower=0,upper=1> fp;
         }
         parameters{
          real<lower=0,upper=1> theta;  
         }
         model {
          theta ~ beta(alpha, beta);
          y ~ binomial(n, theta*(1-fn)+(1-theta)*fp);
         }
         "))
}

input.to.stan <- stan.input()

fit <- stan(model_code = input.to.stan$model, data=input.to.stan$data, 
            init = input.to.stan$inits, pars = input.to.stan$para, 
            iter = 1, chains = input.to.stan$n.chains)
fit2 <- stan(fit=fit, data = input.to.stan$data,
             init=input.to.stan$inits, pars=input.to.stan$para,
             iter = 200000, chains = input.to.stan$n.chains, thin=700)
print(fit2)
```


```{r, fig.width=4, fig.align="center", fig.height=3.75, tidy=TRUE}
fit2coef <- extract(fit2, permuted=T)

postTheta2<- function(y = 5,n = 20,fn = 0.05, fp = 0.02, alpha= 2, beta= 18, theta= seq(0.0, 0.6, 0.001), thetaRV=rvsims(fit2coef$theta)){
post <- (theta^(alpha-1))*((1-theta)^(beta-1))*((theta*(1-fn)+(1-theta)*fp)^y)*((1-theta*(1-fn)-(1-theta)*fp)^(n-y))
post <- post/(sum(post*0.001))
par(mar=c(3,3,1,0.5), mgp=c(1.25,0.125,0), las=1, tck=0.01)
rvhist(thetaRV, freq=F, ylim=c(0, max(post, dbeta(theta, alpha, beta))), xlim=c(0,0.6), xlab="$\\theta$", ylab="$\\pi(\\theta|y)$", main="", breaks=20)
lines(theta, post)
curve(dbeta(x, alpha,beta), add=T, lty=2, col="red")
abline(v=y/n, lty=3, col="blue")
invisible(cbind(theta, post))
}
##tikz(file=paste(plotDIR, "BayesTest3.tex", sep="/"), height=3, width=4.5, standAlone = F)
output <- postTheta2()
legend(x=0.4, y=6.5, col=c("red","black","blue"), lty=c(2,1,3), legend = c("prior", "posterior","data"), 
       bty="n")
### dev.off()

```

If we don't know $f_p$ for sure and we can summarize our uncertainty about the false positive rate using a beta distribution, we can still use the numerical method by calculating the posterior density over a grid of $f_p$ and $\theta$:
```{r, fig.width=4, fig.align="center", fig.height=3.75, tidy=TRUE, message=FALSE}
postTheta2D<- function(y = 5,n = 20,fn = 0.05, fp = seq(0, 0.2, 0.001), alpha= 2, beta= 18, ap=2, bp=100, theta= seq(0.0, 0.6, 0.001)){
  theta_fp <- expand.grid(theta, fp)
  thetaG <- theta_fp[,1]
  fpG <- theta_fp[,2]
  post <- matrix((fpG^(ap-1))*((1-fpG)^(bp-1))*(thetaG^(alpha-1))*((1-thetaG)^(beta-1))*((thetaG*(1-fn)+(1-thetaG)*fpG)^y)*((1-thetaG*(1-fn)-(1-thetaG)*fpG)^(n-y)), nrow=length(theta), ncol=length(fp)) 
par(mar=c(3,3,1,0.5), mgp=c(1.25,0.125,0), tck=0.01)
contour(theta, fp, post, xlab="$\\theta$", ylab="$f_p$", drawlabels = F, nlevels = 15)
invisible()
}
## Figure 3
tikz(file=paste(plotDIR, "BayesTest2D.tex", sep="/"), height=3, width=4.5, standAlone = F)
postTheta2D(fp=seq(0,0.07,0.001), theta=seq(0, 0.4,0.001))
dev.off()
```

When using MCMC, we can further expand the model to situations where $f_n$ and $f_p$ are uncertain.  In other words, we can use beta distributions to model the uncertainty of both false positive and false negative probabilities.  

```{r, message=FALSE, tidy=TRUE, warning=FALSE}
stan.input2 <- function(y=5, n=20, alpha=2, beta=18, an=5, bn=100,  ap=2, bp=100, nchains=4)
{
  inits <- list()
  bugs.data <- list(y=y, n=n, alpha=alpha, beta=beta, an=an, bn=bn, ap=ap, bp=bp)
  for (i in 1:nchains) 
    inits[[i]] <- list(theta=runif(1), fn=runif(1), fp=runif(1))
  para <- c("theta", "fn", "fp")
  return(list(para=para, data=bugs.data, inits=inits, n.chains=nchains, model = "
         data{
          int n;
          int y;
          real alpha;
          real beta;
          real an;
          real bn; 
          real ap;
          real bp; 
         }
         parameters{
          real<lower=0,upper=1> theta;
          real<lower=0,upper=1> fn;  
          real<lower=0,upper=1> fp;  
         }
         model {
          fn ~ beta(an, bn);
          fp ~ beta(ap, bp);
          theta ~ beta(alpha, beta);
          y ~ binomial(n, theta*(1-fn)+(1-theta)*fp);
         }
         "))
}

input.to.stan <- stan.input2()

fitB <- stan(model_code = input.to.stan$model, data=input.to.stan$data, 
            init = input.to.stan$inits, pars = input.to.stan$para, 
            iter = 1, chains = input.to.stan$n.chains)
fitB2 <- stan(fit=fitB, data = input.to.stan$data,
             init=input.to.stan$inits, pars=input.to.stan$para,
             iter = 200000, chains = input.to.stan$n.chains, thin=700)

print(fitB2)

```

Sometimes, we have little knowledge of the test (large uncertainty on false positive and false negative probabilities).   

```{r, message=FALSE, tidy=TRUE, warning=FALSE}
input.to.stan <- stan.input2(an=3, bn=10,  ap=1, bp=1, nchains=nchains)

fitB <- stan(model_code = input.to.stan$model, data=input.to.stan$data, 
            init = input.to.stan$inits, pars = input.to.stan$para, 
            iter = 1, chains = input.to.stan$n.chains)
fitB3 <- stan(fit=fitB, data = input.to.stan$data,
             init=input.to.stan$inits, pars=input.to.stan$para,
             iter = 200000, chains = input.to.stan$n.chains, thin=700)

print(fitB3)
```

## Figure 1
Use a yin-yang symbol to depict the concept

```{r}
## a single yin-yang symbol
yinyang <- function(angle=90, col1=rgb(0.2,0.2,0.2,0.5), col2=rgb(0.8,0.8,0.8,0.5)){
  r <- 5
  ax1 <- seq(0, r, length=100)
  arc1 <- -sqrt(r^2-c(rev(ax1), ax1)^2)
  arc2 <- -sqrt(r^2/4-(-ax1+r/2)^2)
  arc3 <-  sqrt(r^2/4-(ax1-r/2)^2)
  par(pty="s")
  plot(-r:r, xlim=c(-r,r), ylim=c(-r,r), xlab="", ylab="", axes=F, type="n")
  if (angle==90){
    polygon(y=-c(arc1, arc3, arc2), x=c(rev(ax1), -ax1, rev(-ax1), ax1), density=-1, col=col1)
    polygon(x=c(rev(ax1), -ax1, rev(-ax1), ax1), y=c(arc1, arc2, arc3), density = -1, col=col2)
  } else if (angle==180){
    polygon(x=c(arc1, arc2, arc3), y=c(rev(ax1), -ax1, rev(-ax1), ax1), density=-1, col=col1)
    polygon(x=c(-arc1, arc2, arc3), y=c(rev(ax1), -ax1, rev(-ax1), ax1), density=-1, col=col2)
  } else 
    stop("angle must be either 90 or 180")
}

## overlay two symbols
yinyang2 <- function(col1=rgb(0.,0.,0.,1), col2=rgb(1,1,1,1), col3=rgb(0.5,0.5,0.5,0.5), col4=rgb(0.1,0.5,0.4, 0.5)){
  r <- 5
  ax1 <- seq(0, r, length=100)
  arc1 <- -sqrt(r^2-c(rev(ax1), ax1)^2)
  arc2 <- -sqrt(r^2/4-(-ax1+r/2)^2)
  arc3 <-  sqrt(r^2/4-(ax1-r/2)^2)
  par(pty="s")
  plot(-r:r, xlim=c(-r,r), ylim=c(-r,r), xlab="", ylab="", axes=F, type="n")
    polygon(y=-c(arc1, arc3, arc2), x=c(rev(ax1), -ax1, rev(-ax1), ax1), density=-1, col=col1)
    polygon(x=c(rev(ax1), -ax1, rev(-ax1), ax1), y=c(arc1, arc2, arc3), density = -1, col=col2)
    polygon(x=c(arc1, arc2, arc3), y=c(rev(ax1), -ax1, rev(-ax1), ax1), density=-1, col=col3)
    polygon(x=c(-arc1, arc2, arc3), y=c(rev(ax1), -ax1, rev(-ax1), ax1), density=-1, col=col4)
}

tikzDevice::tikz(file="van.tex", height=4, width=6.5, standAlone=F)
par(mfrow=c(1,3), mar=c(0.125,0.125,0.125,0.125), mgp=c(1.25,0.125,0))
yinyang(col1=rgb(0.,0.,0.,1), col2=rgb(1,1,1,1))
text(-2.5, 0, "$p$", col="yellow", cex=1.25)
text(2.5, 0, "$a$", cex=1.25)
title(sub="(i)")
yinyang(angle=180, col1=rgb(0.5,0.5,0.5,0.5), col2=rgb(0.1,0.5,0.4, 0.5))
text(0, 2.5, "$+$", cex=1.25)
text(0, -2.5, "$-$", cex=1.25)
title(sub="(ii)")
yinyang2()
text(-2.5, 1.25, "$p\\&+$", col="yellow", cex=1.25)
text(2.5, -1.25, "$a\\&-$", cex=1.25)
text(3.25, 3., "$p\\&-$", col="yellow", cex=1.25)
text(-3.125, -3.,"$a\\&+$", cex=1.25)
text(1.25, 1.25,"$a\\&+$", cex=1.25)
text(-1.25, -1.25, "$p\\&-$", col="yellow", cex=1.25)
title(sub="(iii)")
dev.off()
```
